{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run epigenomic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Please only use the following versions:\n",
    "`python`: 3.8.16\n",
    "`pacmap`: 0.7.0\n",
    "`lightgbm`: 3.3.5\n",
    "`scikit-learn`: 1.2.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where the data at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mount = '/mnt/d/'\n",
    "input_path = mount + 'MethylScore_v2/Intermediate_Files/'\n",
    "output_path = mount + 'MethylScore_v2/Processed_Data/'\n",
    "\n",
    "nanopore_path = '/mnt/c/Users/fmarc/OneDrive/Desktop/nanopore_processed/pacmap/'\n",
    "\n",
    "sample_name = 'uf_hembank_1830'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nanopore dataset (df_nanopore) contains 331923 columns (5mC nucleotides/probes) and 1 rows (samples).\n",
      " Discovery dataset (df_discovery) contains 331923 columns (5mC nucleotides/probes) and 3307 rows (samples).\n"
     ]
    }
   ],
   "source": [
    "df_nanopore = pd.read_pickle(\n",
    "    nanopore_path + sample_name + '_pacmap_bvalues.pkl').sort_index()\n",
    "\n",
    "print(\n",
    "f' Nanopore dataset (df_nanopore) contains {df_nanopore.shape[1]} \\\n",
    "columns (5mC nucleotides/probes) and {df_nanopore.shape[0]} rows (samples).')\n",
    "\n",
    "df_discovery = pd.read_pickle(\n",
    "    input_path+'3308samples_333059cpgs_withbatchcorrection_bvalues.pkl').sort_index().iloc[:,1:][df_nanopore.columns]\n",
    "\n",
    "print(\n",
    "f' Discovery dataset (df_discovery) contains {df_discovery.shape[1]} \\\n",
    "columns (5mC nucleotides/probes) and {df_discovery.shape[0]-1} rows (samples).')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality with PaCMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters, fit, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaCMAP version: 0.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marchi/projects/AL_atlas_notebooks/.venv_py38/lib/python3.8/site-packages/pacmap/pacmap.py:819: UserWarning: Warning: random state is set to 42\n",
      "  warnings.warn(f'Warning: random state is set to {_RANDOM_STATE}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PaCMAP instance is successfully saved at /mnt/c/Users/fmarc/OneDrive/Desktop/nanopore_processed/pacmap/uf_hembank_1830_pacmap_2d_model_al_atlas.pkl, and the Annoy Index is saved at /mnt/c/Users/fmarc/OneDrive/Desktop/nanopore_processed/pacmap/uf_hembank_1830_pacmap_2d_model_al_atlas.ann.\n",
      "To load the instance again, please do `pacmap.load(/mnt/c/Users/fmarc/OneDrive/Desktop/nanopore_processed/pacmap/uf_hembank_1830_pacmap_2d_model_al_atlas)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marchi/projects/AL_atlas_notebooks/.venv_py38/lib/python3.8/site-packages/pacmap/pacmap.py:819: UserWarning: Warning: random state is set to 42\n",
      "  warnings.warn(f'Warning: random state is set to {_RANDOM_STATE}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PaCMAP instance is successfully saved at /mnt/c/Users/fmarc/OneDrive/Desktop/nanopore_processed/pacmap/uf_hembank_1830_pacmap_5d_model_al_atlas.pkl, and the Annoy Index is saved at /mnt/c/Users/fmarc/OneDrive/Desktop/nanopore_processed/pacmap/uf_hembank_1830_pacmap_5d_model_al_atlas.ann.\n",
      "To load the instance again, please do `pacmap.load(/mnt/c/Users/fmarc/OneDrive/Desktop/nanopore_processed/pacmap/uf_hembank_1830_pacmap_5d_model_al_atlas)`.\n"
     ]
    }
   ],
   "source": [
    "import pacmap\n",
    "print('PaCMAP version:', pacmap.__version__)\n",
    "\n",
    "components_list = [2, 5]\n",
    "for components in components_list:\n",
    "    \n",
    "    reducer = pacmap.PaCMAP(n_components=components, \n",
    "                            n_neighbors=10, \n",
    "                            MN_ratio=0.4, \n",
    "                            FP_ratio=16.0, \n",
    "                            lr=0.1, \n",
    "                            num_iters=5000,\n",
    "                            random_state=42,\n",
    "                            save_tree=True)\n",
    "\n",
    "    # Project the high dimensional dataset into a low-dimensional embedding\n",
    "    embedding_training = reducer.fit_transform(df_discovery.to_numpy(dtype='float16'))\n",
    "\n",
    "    # Save reducer\n",
    "    pacmap.save(reducer, nanopore_path + f'{sample_name}_pacmap_{components}d_model_al_atlas')\n",
    "\n",
    "    # Create column names\n",
    "    cols = ['PaCMAP '+ str(i+1) + f' of {components}' for i in range(components)]\n",
    "\n",
    "    # Turn embedding into dataframe\n",
    "    df_embedding = pd.DataFrame(embedding_training, columns=cols, index=df_discovery.index).to_pickle(\n",
    "        nanopore_path+f'{sample_name}_pacmap_{components}d_embedding.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply models to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply_pacmap_model_to_new_data(df, components):\n",
    "\n",
    "    # Load reducer\n",
    "    reducer = pacmap.load(nanopore_path + f'{sample_name}_pacmap_{components}d_model_al_atlas')\n",
    "\n",
    "    # Project the high dimensional dataset into existing embedding space and return the embedding.\n",
    "    embedding = reducer.transform(df.to_numpy(dtype='float16'))\n",
    "\n",
    "    # Create column names\n",
    "    cols = ['PaCMAP '+ str(i+1) + f' of {components}' for i in range(components)]\n",
    "\n",
    "    # Turn embedding into dataframe\n",
    "    df_embedding = pd.DataFrame(embedding, columns=cols, index=df.index)\n",
    "\n",
    "    return df_embedding\n",
    "\n",
    "# Apply pacmap model to discovery dataset\n",
    "train_2d = pd.read_pickle(nanopore_path+sample_name+'_pacmap_2d_embedding.pkl')\n",
    "train_5d = pd.read_pickle(nanopore_path+sample_name+'_pacmap_5d_embedding.pkl')\n",
    "\n",
    "# # Apply pacmap model to validation dataset\n",
    "test_2d = apply_pacmap_model_to_new_data(df_nanopore, components=2)\n",
    "test_5d = apply_pacmap_model_to_new_data(df_nanopore, components=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge results with clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Join 2d and 5d\n",
    "train = train_2d.join(train_5d)\n",
    "test = test_2d.join(test_5d)\n",
    "\n",
    "# Concatenate train and test\n",
    "train_test = pd.concat([train, test])\n",
    "\n",
    "# Read clinical data\n",
    "clinical_data = pd.read_excel(input_path+'clinical_data.xlsx', index_col=0)\n",
    "\n",
    "# Join train_test with clinical data\n",
    "df = train_test.join(clinical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_5d = pd.concat([train_5d, test_5d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_2d = pd.concat([train_2d, test_2d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select samples Px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1465 samples removed. 1844 samples remaining.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop the samples with missing labels for the selected column\n",
    "df_px = df[~df['Vital Status'].isna()]\n",
    "\n",
    "df_px2 = df_px.copy()\n",
    "\n",
    "# # Exclude the `Blood Derived Normal`and `Bone Marrow Normal` from `Sample Type`\n",
    "# df_px2 = df_px[~df_px['Sample Type'].isin([\n",
    "#                                             'Relapse', 'Recurrent Blood Derived Cancer - Bone Marrow',\n",
    "#                                             'Recurrent Blood Derived Cancer - Peripheral Blood',\n",
    "#                                             'Blood Derived Normal', 'Bone Marrow Normal'\n",
    "# ])]\n",
    "\n",
    "# print the number of samples dropped and the amount remaining\n",
    "print(df.shape[0]-df_px2.shape[0], 'samples removed.'\\\n",
    ", df_px2.shape[0], 'samples remaining.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select samples Dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864 samples removed. 2445 samples remaining.\n"
     ]
    }
   ],
   "source": [
    "# drop the samples with missing labels for the ELN AML 2022 Diagnosis\n",
    "df_dx = df[~df['WHO 2022 Diagnosis'].isna()]\n",
    "\n",
    "# exclude the classes with fewer than 10 samples\n",
    "df_dx2 = df_dx[~df_dx['WHO 2022 Diagnosis'].isin([\n",
    "                                       'MPAL with t(v;11q23.3)/KMT2A-r',\n",
    "                                       'B-ALL with hypodiploidy',\n",
    "                                       'AML with t(16;21); FUS::ERG',\n",
    "                                       'AML with t(9;22); BCR::ABL1'\n",
    "                                       ])]\n",
    "\n",
    "# print the number of samples dropped and the amount remaining\n",
    "print(df.shape[0]-df_dx2.shape[0], 'samples removed.'\\\n",
    ", df_dx2.shape[0], 'samples remaining.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2445, 5) X_test shape: (0, 5)\n",
      "X_train shape: (1844, 5) X_test shape: (0, 5)\n"
     ]
    }
   ],
   "source": [
    "def custom_train_test_split(df, feature_columns, target_column, split_column):\n",
    "\n",
    "    X = df[feature_columns].to_numpy(dtype='float16')\n",
    "    y = df[target_column].to_numpy()\n",
    "\n",
    "    train_mask = df[split_column] == 'Train Sample'\n",
    "    test_mask = df[split_column] == 'Test Sample'\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Execution\n",
    "X_train_dx, X_test_dx, y_train_dx, y_test_dx = custom_train_test_split(df_dx2, test_5d.columns,'WHO 2022 Diagnosis', 'Train-Test')\n",
    "X_train_px, X_test_px, y_train_px, y_test_px = custom_train_test_split(df_px2, test_5d.columns,'Vital Status', 'Train-Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'class_weight': 'balanced', 'reg_alpha': 1, 'reg_lambda': 1}\n",
      "Best parameters: {'class_weight': 'balanced', 'reg_alpha': 1, 'reg_lambda': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "import joblib\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# from source.data_visualization import plot_confusion_matrix_stacked\n",
    "\n",
    "\n",
    "def benchmark_classifier(X_train, y_train):\n",
    "\n",
    "    param_grid = {\n",
    "    # 'num_leaves': [ 5, 6, 7, 8, 9, 10], # number of leaves in full tree\n",
    "    # 'max_depth': [3,4,5,6,7,8,9,10],  # maximum depth of a tree\n",
    "    # 'learning_rate': [0.01],  # learning rate\n",
    "    # 'n_estimators': [50, 100, 500],  # number of trees (or rounds)\n",
    "    'reg_alpha': [1],  # L1 regularization\n",
    "    'reg_lambda': [1],  # L2 regularization\n",
    "    'class_weight': ['balanced'],  # weights associated with classes\n",
    "    \n",
    "    }\n",
    "\n",
    "    # Initialize the LGBM Classifier with regularization\n",
    "    lgbm = LGBMClassifier(random_state=42, n_jobs=-1) \n",
    "                          \n",
    "    # Perform grid search with stratified cross-validation\n",
    "    grid_search = GridSearchCV(lgbm, param_grid, cv=10, n_jobs=-1,\n",
    "                               scoring='roc_auc_ovr_weighted')\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    # Get the best model\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    return clf\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "\n",
    "# Benchmark, train\n",
    "lgbm_dx_model = benchmark_classifier(X_train_dx, y_train_dx)\n",
    "lgbm_px_model = benchmark_classifier(X_train_px, y_train_px)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AML Epigenomic Risk</th>\n",
       "      <th>AML Epigenomic Risk P(High Risk)</th>\n",
       "      <th>AL Epigenomic Phenotype</th>\n",
       "      <th>P(AML with t(v;11q23); KMT2A-r)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>uf_hembank_1830</th>\n",
       "      <td>High</td>\n",
       "      <td>0.881</td>\n",
       "      <td>AML with t(v;11q23); KMT2A-r</td>\n",
       "      <td>0.974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AML Epigenomic Risk  AML Epigenomic Risk P(High Risk)  \\\n",
       "uf_hembank_1830                High                             0.881   \n",
       "\n",
       "                      AL Epigenomic Phenotype  P(AML with t(v;11q23); KMT2A-r)  \n",
       "uf_hembank_1830  AML with t(v;11q23); KMT2A-r                            0.974  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_predictions(df, classifier, model_name):\n",
    "\n",
    "    # ignore sklearn warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Select necessary columns\n",
    "    df_features = df.copy()\n",
    "\n",
    "    # Predict using the selected columns\n",
    "    predictions = classifier.predict(df_features)\n",
    "\n",
    "    # Predict probabilities using the selected columns\n",
    "    probabilities = classifier.predict_proba(df_features)\n",
    "\n",
    "    # Convert predictions to a Series with the same index as df_features\n",
    "    predictions_series = pd.Series(predictions, index=df_features.index, name=model_name)\n",
    "\n",
    "    # Convert probabilities to a DataFrame with the same index as df_features and the same columns as the classes\n",
    "    probabilities_df = pd.DataFrame(probabilities, index=df_features.index, columns=classifier.classes_).round(3)\n",
    "\n",
    "    # Add \" - predict_proba\" to the column names\n",
    "    probabilities_df.columns ='P(' + probabilities_df.columns + ')'\n",
    "\n",
    "    # Transform classes of the predictions into integers based on unique values in the classes\n",
    "    probabilities_df[model_name + '_int'] = predictions_series.map({c: i for i, c in enumerate(classifier.classes_)})\n",
    "\n",
    "    # Join predictions with the original DataFrame (already indexed)\n",
    "    df_joined = predictions_series.to_frame().join(probabilities_df)\n",
    "\n",
    "    return df_joined\n",
    "\n",
    "# Execution\n",
    "df_pred_px = save_predictions(df=train_test_5d, classifier=lgbm_px_model, model_name='AML Epigenomic Risk')\n",
    "df_pred_dx = save_predictions(df=train_test_5d, classifier=lgbm_dx_model, model_name='AL Epigenomic Phenotype')\n",
    "\n",
    "# Map the classes to more desirable labels (low and high risk)\n",
    "df_pred_px['AML Epigenomic Risk'] = df_pred_px['AML Epigenomic Risk'].map({'Alive': 'Low', 'Dead': 'High'})\n",
    "df_pred_px = df_pred_px.rename(columns={'P(Alive)': 'AML Epigenomic Risk P(Low Risk)', 'P(Dead)': 'AML Epigenomic Risk P(High Risk)'})\n",
    "\n",
    "# Join predictions with clinical data\n",
    "df_combined = train_test_2d.join(train_test_5d).join(df_pred_px).join(df_pred_dx)\n",
    "\n",
    "\n",
    "df_combined.iloc[-1:,:][['AML Epigenomic Risk', 'AML Epigenomic Risk P(High Risk)', 'AL Epigenomic Phenotype', f'P({df_combined.iloc[-1:,:][\"AL Epigenomic Phenotype\"].item()})']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EWASCox-Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous score cut at the value of 0.4934\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>EWASCox_OS_48CpGs</th>\n",
       "      <th>EWASCox_OS_48CpGs Categorical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>uf_hembank_1830</th>\n",
       "      <td>0.094916</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name             EWASCox_OS_48CpGs EWASCox_OS_48CpGs Categorical\n",
       "uf_hembank_1830           0.094916                           Low"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from source.cox_lasso import *\n",
    "\n",
    "raw_coefs = pd.read_csv(output_path + 'multivariate_cox_lasso/ewas_cog_os_raw_coefs_newrisk.csv', index_col=0)\n",
    "\n",
    "mean_coefs = set_cutoff(coefs=raw_coefs,threshold=0.99)\n",
    "\n",
    "df_validation = df_nanopore[mean_coefs.index]\n",
    "\n",
    "df_validation_transformed = df_validation.replace(1, 0.999).replace(0, 0.001)\n",
    "\n",
    "def beta2m(val):\n",
    "    '''Transfrom beta-values into m-values'''\n",
    "    return math.log2(val/(1-val))\n",
    "\n",
    "x_test_m = df_validation_transformed.apply(np.vectorize(beta2m))\n",
    "\n",
    "def standardize_data(df, reference_df):\n",
    "    \"\"\"Standardize data using mean and standard deviation of reference dataset\"\"\"\n",
    "\n",
    "    # Keep only columns that are in both datasets\n",
    "    reference_df = reference_df.loc[:, df.columns]\n",
    "\n",
    "    # Standardize data\n",
    "    df_z = (df - reference_df.mean()) / reference_df.std()\n",
    "\n",
    "    return df_z\n",
    "\n",
    "# Read top CpGs selected from previous code file (univariate cox-ph EWAS)\n",
    "ewas_top_cpgs = pd.read_csv(output_path+'ewas_dmr/ewas_top_cpgs_os.csv', index_col=0)\n",
    "\n",
    "# Standardize data\n",
    "x_test_m_z = standardize_data(df= x_test_m, reference_df= ewas_top_cpgs)\n",
    "\n",
    "score_name = 'EWASCox_OS_48CpGs'\n",
    "\n",
    "df_test, threshold = generate_coxph_score(coef_mean=mean_coefs,\n",
    "                                        x=x_test_m_z,\n",
    "                                        df=df_validation_transformed,\n",
    "                                        score_name=score_name,\n",
    "                                        train_test=0.4934,\n",
    "                                        rpart_outcome='os.time')\n",
    "\n",
    "df_validation_transformed[['EWASCox_OS_48CpGs','EWASCox_OS_48CpGs Categorical']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data for uf_hembank_1830 saved as /mnt/d/MethylScore_v2/Processed_Data/uf_hembank_1830_processed.pkl\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_nanopore = df_combined.join(df_validation_transformed[['EWASCox_OS_48CpGs','EWASCox_OS_48CpGs Categorical']])\n",
    "except:\n",
    "    df_nanopore = df_combined\n",
    "\n",
    "# df_nanopore['Train-Test'] = 'Long-read Nanopore sequencing'\n",
    "# df_nanopore['Clinical Trial'] = 'UF Hem Bank'\n",
    "# df_nanopore['Patient_ID'] = sample_name\n",
    "# df_nanopore['Hematopoietic Entity'] = np.nan\n",
    "# df_nanopore['WHO 2022 Diagnosis'] =  np.nan\n",
    "# df_nanopore['Vital Status'] = np.nan\n",
    "# df_nanopore['Risk Group AAML1831'] = np.nan\n",
    "    \n",
    "# Merge with clinical data\n",
    "df_nanopore = df_nanopore.join(clinical_data)\n",
    "\n",
    "df_nanopore.to_pickle(output_path + sample_name + '_processed.pkl')\n",
    "\n",
    "# print save message\n",
    "print(f'Processed data for {sample_name} saved as {output_path + sample_name + \"_processed.pkl\"}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Francisco_Marchi@Lamba_Lab_UF\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.18\n",
      "IPython version      : 8.12.2\n",
      "\n",
      "numpy   : 1.24.3\n",
      "pandas  : 2.0.2\n",
      "pacmap  : 0.7.0\n",
      "sklearn : 1.2.2\n",
      "lightgbm: 3.3.5\n",
      "\n",
      "Compiler    : GCC 11.4.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.133.1-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 32\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# watermark with all libraries used in this notebook\n",
    "%watermark -v -p numpy,pandas,pacmap,sklearn,lightgbm -a Francisco_Marchi@Lamba_Lab_UF -d -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Updating to python 3.10 caused substantial delays in lightgbm, so please use the following versions:\n",
    "`python`: 3.8.16\n",
    "`pacmap`: 0.7.0\n",
    "`lightgbm`: 3.3.5\n",
    "`scikit-learn`: 1.2.2\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
