{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithms\n",
    "\n",
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where the data at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# from source.pacmap_functions import *\n",
    "\n",
    "mount = '/mnt/h/'\n",
    "input_path = mount + 'MethylScore/Intermediate_Files/'\n",
    "output_path = mount + 'MethylScore/Processed_Data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Discovery dataset (df_discovery) contains 319558 columns (5mC nucleotides/probes) and 3308 rows (samples).\n",
      " Validation dataset (df_validation) contains 319558 columns (5mC nucleotides/probes) and 201 rows (samples).\n"
     ]
    }
   ],
   "source": [
    "# read df_discovery and df_validation\n",
    "df_discovery = pd.read_pickle(\n",
    "    input_path+'3308samples_333059cpgs_withbatchcorrection_bvalues.pkl').sort_index()\n",
    "\n",
    "df_validation = pd.read_pickle(\n",
    "    input_path+'201samples_357839cpgs_withbatchcorrection_bvalues.pkl').sort_index()\n",
    "\n",
    "# use overlapping features between df_discovery and df_validation\n",
    "common_features = [x for x in df_discovery.columns if x in df_validation.columns]\n",
    "\n",
    "# apply `common_features` to both df_discovery and df_validation\n",
    "df_discovery = df_discovery[common_features]\n",
    "df_validation = df_validation[common_features]\n",
    "\n",
    "print(\n",
    "f' Discovery dataset (df_discovery) contains {df_discovery.shape[1]} \\\n",
    "columns (5mC nucleotides/probes) and {df_discovery.shape[0]} rows (samples).')\n",
    "\n",
    "print(\n",
    "f' Validation dataset (df_validation) contains {df_validation.shape[1]} \\\n",
    "columns (5mC nucleotides/probes) and {df_validation.shape[0]} rows (samples).')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality with PaCMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters, fit, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaCMAP version: 0.7.1\n"
     ]
    }
   ],
   "source": [
    "import pacmap\n",
    "print('PaCMAP version:', pacmap.__version__)\n",
    "\n",
    "# components_list = [2, 5]\n",
    "# for components in components_list:\n",
    "\n",
    "#     # Initialize the instance\n",
    "#     reducer = pacmap.PaCMAP(n_components=components, n_neighbors=15, MN_ratio=0.4, FP_ratio=16.0, \n",
    "#                             random_state=42, lr=0.1, num_iters=5000, save_tree=True)\n",
    "\n",
    "#     # Project the high dimensional dataset into a low-dimensional embedding\n",
    "#     embedding_training = reducer.fit(df_discovery.to_numpy(dtype='float16'))\n",
    "\n",
    "#     # Save reducer\n",
    "#     pacmap.save(reducer, f'../models/pacmap_{components}d_model_al_atlas')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models, transform, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_pacmap_model_to_new_data(df, components):\n",
    "\n",
    "    # Load reducer\n",
    "    reducer = pacmap.load(f'../models/pacmap_{components}d_model_al_atlas')\n",
    "\n",
    "    # Project the high dimensional dataset into existing embedding space and return the embedding.\n",
    "    embedding = reducer.transform(df.to_numpy(dtype='float16'))\n",
    "\n",
    "    # Create column names\n",
    "    cols = ['PaCMAP ' + str(i+1) for i in range(components)]\n",
    "\n",
    "    # Turn embedding into dataframe\n",
    "    df_embedding = pd.DataFrame(embedding, columns=cols)\n",
    "\n",
    "    return df_embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = 2\n",
    "\n",
    "# Load reducer\n",
    "reducer = pacmap.load(f'../models/pacmap_{components}d_model_al_atlas')\n",
    "\n",
    "# Project the high dimensional dataset into existing embedding space and return the embedding.\n",
    "embedding_training = reducer.transform(df_discovery.to_numpy(dtype='float16'))\n",
    "embedding_validation = reducer.transform(df_validation.to_numpy(dtype='float16'))\n",
    "\n",
    "cols = ['PaCMAP ' + str(i+1) for i in range(components)]\n",
    "\n",
    "# Turn embedding_training and embedding_validation into dataframes\n",
    "embedding_training = pd.DataFrame(embedding_training, index=df_discovery.index,)\n",
    "embedding_validation = pd.DataFrame(embedding_validation, index=df_validation.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn embedding_training and embedding_validation into dataframes\n",
    "embedding_training = pd.DataFrame(embedding_training, index=df_discovery.index,)\n",
    "embedding_validation = pd.DataFrame(embedding_validation, index=df_validation.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0003fe29-7b8f-4ef1-b9bc-40306205f1fd_noid</th>\n",
       "      <td>13.322694</td>\n",
       "      <td>-13.645694</td>\n",
       "      <td>0.077213</td>\n",
       "      <td>16.639061</td>\n",
       "      <td>-7.293494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0031efba-f564-4fff-bd7b-2b97f37218c1_noid</th>\n",
       "      <td>-11.715875</td>\n",
       "      <td>5.200182</td>\n",
       "      <td>9.739075</td>\n",
       "      <td>10.687033</td>\n",
       "      <td>12.903965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0037ec75-bb9e-4dbb-a2d9-de1f9bfd2362_noid</th>\n",
       "      <td>13.497449</td>\n",
       "      <td>-12.308421</td>\n",
       "      <td>1.874104</td>\n",
       "      <td>-13.578052</td>\n",
       "      <td>-15.560556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>003d9efe-90a1-42cf-84c4-03fbabefe60b_noid</th>\n",
       "      <td>-18.319000</td>\n",
       "      <td>-14.954743</td>\n",
       "      <td>-8.929393</td>\n",
       "      <td>-15.147971</td>\n",
       "      <td>-1.855355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>004c953f-d999-4d82-898d-c091e692df3c_noid</th>\n",
       "      <td>-13.724726</td>\n",
       "      <td>-2.158764</td>\n",
       "      <td>13.818226</td>\n",
       "      <td>11.145183</td>\n",
       "      <td>13.025648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fd4e4be4-3306-4037-b4e9-7886283243ed_noid</th>\n",
       "      <td>13.695102</td>\n",
       "      <td>-7.683386</td>\n",
       "      <td>21.030201</td>\n",
       "      <td>-1.646669</td>\n",
       "      <td>14.846841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe185655-3aa5-4c5c-9dfd-630e0ee71710_noid</th>\n",
       "      <td>-13.250676</td>\n",
       "      <td>12.487949</td>\n",
       "      <td>18.722261</td>\n",
       "      <td>-2.635083</td>\n",
       "      <td>-19.984118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff7f7d14-5fb7-44bc-b67a-d07610873330_noid</th>\n",
       "      <td>-11.935309</td>\n",
       "      <td>3.207566</td>\n",
       "      <td>11.019547</td>\n",
       "      <td>10.726985</td>\n",
       "      <td>13.958644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff87c237-583a-4aa1-880c-7d77d00ba0a6_noid</th>\n",
       "      <td>11.341358</td>\n",
       "      <td>-13.743374</td>\n",
       "      <td>-1.251140</td>\n",
       "      <td>14.473524</td>\n",
       "      <td>0.673762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffeac099-79b4-4fbf-a283-275d31baf531_noid</th>\n",
       "      <td>-14.248646</td>\n",
       "      <td>-0.420476</td>\n",
       "      <td>2.248876</td>\n",
       "      <td>27.462049</td>\n",
       "      <td>-2.691683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3308 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0          1          2  \\\n",
       "0003fe29-7b8f-4ef1-b9bc-40306205f1fd_noid  13.322694 -13.645694   0.077213   \n",
       "0031efba-f564-4fff-bd7b-2b97f37218c1_noid -11.715875   5.200182   9.739075   \n",
       "0037ec75-bb9e-4dbb-a2d9-de1f9bfd2362_noid  13.497449 -12.308421   1.874104   \n",
       "003d9efe-90a1-42cf-84c4-03fbabefe60b_noid -18.319000 -14.954743  -8.929393   \n",
       "004c953f-d999-4d82-898d-c091e692df3c_noid -13.724726  -2.158764  13.818226   \n",
       "...                                              ...        ...        ...   \n",
       "fd4e4be4-3306-4037-b4e9-7886283243ed_noid  13.695102  -7.683386  21.030201   \n",
       "fe185655-3aa5-4c5c-9dfd-630e0ee71710_noid -13.250676  12.487949  18.722261   \n",
       "ff7f7d14-5fb7-44bc-b67a-d07610873330_noid -11.935309   3.207566  11.019547   \n",
       "ff87c237-583a-4aa1-880c-7d77d00ba0a6_noid  11.341358 -13.743374  -1.251140   \n",
       "ffeac099-79b4-4fbf-a283-275d31baf531_noid -14.248646  -0.420476   2.248876   \n",
       "\n",
       "                                                   3          4  \n",
       "0003fe29-7b8f-4ef1-b9bc-40306205f1fd_noid  16.639061  -7.293494  \n",
       "0031efba-f564-4fff-bd7b-2b97f37218c1_noid  10.687033  12.903965  \n",
       "0037ec75-bb9e-4dbb-a2d9-de1f9bfd2362_noid -13.578052 -15.560556  \n",
       "003d9efe-90a1-42cf-84c4-03fbabefe60b_noid -15.147971  -1.855355  \n",
       "004c953f-d999-4d82-898d-c091e692df3c_noid  11.145183  13.025648  \n",
       "...                                              ...        ...  \n",
       "fd4e4be4-3306-4037-b4e9-7886283243ed_noid  -1.646669  14.846841  \n",
       "fe185655-3aa5-4c5c-9dfd-630e0ee71710_noid  -2.635083 -19.984118  \n",
       "ff7f7d14-5fb7-44bc-b67a-d07610873330_noid  10.726985  13.958644  \n",
       "ff87c237-583a-4aa1-880c-7d77d00ba0a6_noid  14.473524   0.673762  \n",
       "ffeac099-79b4-4fbf-a283-275d31baf531_noid  27.462049  -2.691683  \n",
       "\n",
       "[3308 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load output from PaCMAP\n",
    "\n",
    "Reloading it here to allow for running the cells below without running the PaCMAP code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "n_components = 5\n",
    "\n",
    "# Create a list of strings containing \"PaCMAP \" + the number of components\n",
    "components = [f'PaCMAP {i}' for i in range(1, n_components+1)]\n",
    "\n",
    "# Load pacmap output data\n",
    "df = pd.read_csv(output_path+f'pacmap_output/pacmap_{n_components}d_model_dx_al.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select samples Px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop the samples with missing labels for the selected column\n",
    "df_px = df[~df['Vital Status'].isna()]\n",
    "\n",
    "df_px2 = df_px.copy()\n",
    "\n",
    "# # Exclude the `Blood Derived Normal`and `Bone Marrow Normal` from `Sample Type`\n",
    "# df_px2 = df_px[~df_px['Sample Type'].isin(['Relapse', 'Recurrent Blood Derived Cancer - Bone Marrow',\n",
    "#                                             'Recurrent Blood Derived Cancer - Peripheral Blood',\n",
    "#                                             'Blood Derived Normal', 'Bone Marrow Normal'])]\n",
    "\n",
    "# print the number of samples dropped and the amount remaining\n",
    "print(df.shape[0]-df_px2.shape[0], 'samples removed.'\\\n",
    ", df_px2.shape[0], 'samples remaining.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select samples Dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# drop the samples with missing labels for the ELN AML 2022 Diagnosis\n",
    "df_dx = df[~df['WHO 2022 Diagnosis'].isna()]\n",
    "\n",
    "# exclude the classes with fewer than 10 samples\n",
    "df_dx2 = df_dx[~df_dx['WHO 2022 Diagnosis'].isin([\n",
    "                                    'AML with t(1;22); RBM15::MKL1',\n",
    "                                       'MPAL with t(v;11q23.3)/KMT2A-r',\n",
    "                                       'B-ALL with hypodiploidy',\n",
    "                                       'AML with t(16;21); FUS::ERG',\n",
    "                                       'AML with t(9;22); BCR::ABL1'\n",
    "                                       ])]\n",
    "\n",
    "# print the number of samples dropped and the amount remaining\n",
    "print(df.shape[0]-df_dx2.shape[0], 'samples removed.'\\\n",
    ", df_dx2.shape[0], 'samples remaining.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_test_split(df, feature_columns, target_column, split_column):\n",
    "\n",
    "    X = df[feature_columns].to_numpy()\n",
    "    y = df[target_column].to_numpy()\n",
    "\n",
    "    train_mask = df[split_column] == 'Train Sample'\n",
    "    test_mask = df[split_column] == 'Test Sample'\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Execution\n",
    "X_train_dx, X_test_dx, y_train_dx, y_test_dx = custom_train_test_split(df_dx2, components,'WHO 2022 Diagnosis', 'Train-Test')\n",
    "X_train_px, X_test_px, y_train_px, y_test_px = custom_train_test_split(df_px2, components,'Vital Status', 'Train-Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define confusion matrix plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_stacked(clf, x_train, y_train, x_test, y_test, \n",
    "                                  title='', \n",
    "                                  tick_fontsize=10, label_fontsize=10,\n",
    "                                  figsize=(10, 5)):\n",
    "\n",
    "    sns.set_theme(style='white')\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=figsize, sharey=True)\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.05)  # Adjust the width space\n",
    "\n",
    "    for i, (ax, x, y, subset) in enumerate(zip(axs, [x_train, x_test], [y_train, y_test],\n",
    "                                                ['Train with 10-fold CV', 'Validation'])):\n",
    "        predictions = clf.predict(x)\n",
    "        print(f'Overall accuracy score in {subset}: {accuracy_score(y, predictions):.3f}')\n",
    "        cm = confusion_matrix(y, predictions, labels=clf.classes_, normalize='true')\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "        disp.plot(cmap='Blues', values_format='.2f', xticks_rotation='vertical', colorbar=False, ax=ax)\n",
    "    \n",
    "\n",
    "        # Set font size of the numbers inside the confusion matrix\n",
    "        for texts in disp.text_:\n",
    "            for text in texts:\n",
    "                text.set_fontsize(label_fontsize)\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=tick_fontsize)\n",
    "        ax.set_title(subset + ', n=' + str(len(x)), fontsize=label_fontsize+1, pad=10)\n",
    "        ax.set_xlabel('Predicted label', fontsize=label_fontsize+1)\n",
    "\n",
    "        # Add y labels\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('True label', fontsize=label_fontsize+1)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "\n",
    "        # remove x tick labels\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "\n",
    "def benchmark_classifier(X_train, y_train, X_test, y_test, figsize=(16,8)):\n",
    "\n",
    "    param_grid = {\n",
    "    'num_leaves': [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20], # number of leaves in full tree\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],  # maximum depth of a tree\n",
    "    # 'learning_rate': [0.001, 0.01, 0.015, 0.02, 0.1],  # learning rate\n",
    "    # 'n_estimators': [10, 50, 100, 200],  # number of trees (or rounds)\n",
    "    }\n",
    "\n",
    "    # Initialize the LGBM Classifier with regularization\n",
    "    lgbm = LGBMClassifier(random_state=42, n_jobs=-1, reg_alpha=1.0, reg_lambda=1.0)\n",
    "\n",
    "    # Perform grid search with stratified cross-validation\n",
    "    grid_search = GridSearchCV(lgbm, param_grid, cv=10, n_jobs=-1, scoring='roc_auc_ovr_weighted')\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    # Get the best model\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    plot = plot_confusion_matrix_stacked(clf, X_train, y_train, X_test, y_test,\n",
    "                                  tick_fontsize=7, label_fontsize=7, figsize=figsize)\n",
    "    return clf, plot\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "\n",
    "# Benchmark, train\n",
    "lgbm_dx_model, plot = benchmark_classifier(X_train_dx, y_train_dx, X_test_dx, y_test_dx)\n",
    "lgbm_px_model, plot = benchmark_classifier(X_train_px, y_train_px, X_test_px, y_test_px, figsize=(5,3))\n",
    "\n",
    "# Save models\n",
    "joblib.dump(lgbm_px_model,'../models/lgbm_px_model.pkl')\n",
    "joblib.dump(lgbm_dx_model,'../models/lgbm_dx_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(df, classifier, model_name):\n",
    "\n",
    "    # ignore sklearn warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Select necessary columns and set the index\n",
    "    df_features = df.set_index('index')[['PaCMAP 1', 'PaCMAP 2', 'PaCMAP 3', 'PaCMAP 4', 'PaCMAP 5']]\n",
    "\n",
    "    # Predict using the selected columns\n",
    "    predictions = classifier.predict(df_features)\n",
    "\n",
    "    # Predict probabilities using the selected columns\n",
    "    probabilities = classifier.predict_proba(df_features)\n",
    "\n",
    "    # Convert predictions to a Series with the same index as df_features\n",
    "    predictions_series = pd.Series(predictions, index=df_features.index, name=model_name)\n",
    "\n",
    "    # Convert probabilities to a DataFrame with the same index as df_features and the same columns as the classes\n",
    "    probabilities_df = pd.DataFrame(probabilities, index=df_features.index, columns=classifier.classes_).round(3)\n",
    "\n",
    "    # Add \" - predict_proba\" to the column names\n",
    "    probabilities_df.columns ='P(' + probabilities_df.columns + ')'\n",
    "\n",
    "    # # Create a new column named \"Confidence\" and set it to True if the max probability is >= confidence_threshold\n",
    "    # probabilities_df['Confidence >=' + str(confidence_threshold)] = probabilities_df.max(axis=1) >= confidence_threshold\n",
    "\n",
    "    # Transform classes of the predictions into integers based on unique values in the classes\n",
    "    probabilities_df[model_name + '_int'] = predictions_series.map({c: i for i, c in enumerate(classifier.classes_)})\n",
    "\n",
    "    # Join predictions with the original DataFrame (already indexed)\n",
    "    df_joined = predictions_series.to_frame().join(probabilities_df)\n",
    "\n",
    "    return df_joined\n",
    "\n",
    "# Execution\n",
    "df_pred_px = save_predictions(df=df, classifier=lgbm_px_model, model_name='AML Epigenomic Risk')\n",
    "df_pred_dx = save_predictions(df=df, classifier=lgbm_dx_model, model_name='AL Epigenomic Phenotype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df.join(df_pred_px).join(df_pred_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other benchmarked classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'multi_class': ['one_vs_rest'], \n",
    "# }\n",
    "\n",
    "# # Initialize the Gaussian Process Classifier\n",
    "# gpc = GaussianProcessClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Perform grid search with cross-validation\n",
    "# grid_search = GridSearchCV(gpc, param_grid, cv=10, n_jobs=-1, scoring='roc_auc_ovr_weighted')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Fit and predict using the best estimator\n",
    "# gpc_px_model = grid_search.best_estimator_\n",
    "\n",
    "# y_pred = gpc_px_model.predict(X_test)  # Predict using the best model\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# # Fit and predict using the best estimator\n",
    "# gpc_px_model = grid_search.best_estimator_\n",
    "\n",
    "# y_pred_train = gpc_px_model.predict(X_train)  # Predict using the best model\n",
    "# y_pred_test = gpc_px_model.predict(X_test)  # Predict using the best model\n",
    "\n",
    "\n",
    "# print(f'Overall accuracy score with best estimator in train: {accuracy_score(y_train, y_pred_train):.3f}')\n",
    "# print(f'Overall accuracy score with best estimator in test: {accuracy_score(y_test, y_pred_test):.3f}')\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plot_confusion_matrix_stacked(gpc_px_model, X_train, y_train, X_test, y_test,\n",
    "#                               tick_fontsize=7, label_fontsize=7, figsize = (5,3))\n",
    "\n",
    "# # you shall save the model\n",
    "# pickle.dump(gpc_px_model, open('../models/gpc_px_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# watermark with all libraries used in this notebook\n",
    "%watermark -v -p numpy,pandas,bokeh,pacmap,itables -a Francisco_Marchi@Lamba_Lab_UF -d -m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
