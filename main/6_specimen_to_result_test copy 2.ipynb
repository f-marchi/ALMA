{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonize ONT reads with array reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Converting CRAM to FASTQ (or basecall straight into fastq and skip this step):\n",
    "\n",
    "    `samtools fastq -@ <num_threads> input.cram > output.fastq`\n",
    "\n",
    "2. Minimap2 aligment using ONT benchmark parameters ([see here](https://github.com/lh3/minimap2/issues/1127#issuecomment-1977169961)):\n",
    "\n",
    "    `minimap2 -x map-ont -t <num_threads> reference_genome.fasta output.fastq > aligned.sam -k19 -w 19 -U50,500 -g10k`\n",
    "\n",
    "    or \n",
    "\n",
    "    `minimap2 -x map-ont -t 20 ../genome_references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna  uf_hembank_<ID>.fastq > aligned.sam -k19 -w 19 -U50,500 -g10k`\n",
    "\n",
    "    or \n",
    "\n",
    "    `minimap2 -ax lr:hq -t 20 ../genome_references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna uf_hembank_1829.fastq > aligned2.sam`\n",
    "\n",
    "3. Modkit\n",
    "    \n",
    "    `modkit pileup uf_hembank_1852.bam uf_hembank_1852_pacmap.bed --combine-mods --no-filtering -t 32 --combine-strands --cpg --ref GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --include-bed pacmap_reference.bed `\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where data at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mount = '/mnt/e/'\n",
    "# input_path = mount + 'MethylScore/Nanopore_data/Kasumi1-naive-p2solo.bed.gz'\n",
    "# input_path = '/mnt/h/UF_HemBank_1852/SAMPLE_ungrouped.wf_mods.bedmethyl.gz'\n",
    "# input_path = '/mnt/h/epi2me_analyses/uf_hembank_1832_wf-human-variation_01HQG462AJKW6RH7XCFBQPD54E/output/UF_HemBank_1832_ungrouped.wf_mods.bedmethyl.gz'\n",
    "input_path = '~/projects/modkit_runs/kasumi1_naive/kasumi1_naive_ungrouped.wf_mods.bedmethyl.gz'\n",
    "reference_path = mount + 'genome_references/Illumina_methylation_arrays/EPIC.hg38.manifest.tsv.gz'\n",
    "output_path = mount + 'MethylScore_v2/Processed_Data/'\n",
    "\n",
    "sample_name = 'kasumi1_naive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonize probes from EPIC array with nanopore methylation calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read df_discovery and df_validation\n",
    "# df_discovery = pd.read_pickle(mount+'MethylScore/Intermediate_Files/'+'3308samples_333059cpgs_withbatchcorrection_bvalues.pkl').sort_index().iloc[:,1:]\n",
    "# array_reference = pd.read_csv('/mnt/d/genome_references/Illumina_methylation_arrays/EPIC.hg38.manifest.tsv.gz', sep='\\t', compression='gzip',\n",
    "#                               usecols=['CpG_chrm','CpG_beg','CpG_end','Probe_ID']).set_index('Probe_ID')\n",
    "\n",
    "# pacmap_reference = array_reference.loc[df_discovery.columns]\n",
    "\n",
    "# # remove `.0` from `CpG_beg` and `CpG_end` and coordinate\n",
    "# pacmap_reference['CpG_beg'] = pacmap_reference['CpG_beg'].astype(int)\n",
    "# pacmap_reference['CpG_end'] = pacmap_reference['CpG_end'].astype(int)\n",
    "\n",
    "# # combine `CpG_chrm` and `CpG_beg` to create `coordinate`\n",
    "# pacmap_reference['coordinate'] = pacmap_reference['CpG_chrm'].astype(str) + ':' + pacmap_reference['CpG_beg'].astype(str)\n",
    "\n",
    "# pacmap_reference.to_pickle(mount+'MethylScore_v2/Intermediate_Files/'+'pacmap_probe_reference.pkl')\n",
    "\n",
    "# load the reference data\n",
    "pacmap_reference = pd.read_pickle(mount+'MethylScore_v2/Intermediate_Files/'+'pacmap_probe_reference.pkl').reset_index().set_index('coordinate')\n",
    "\n",
    "# Save `pacmap_reference` as a .bed file\n",
    "# pacmap_reference[['CpG_chrm', 'CpG_beg', 'CpG_end',]].to_csv('~/projects/modkit_runs/pacmap_reference.bed', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to be used for the input data\n",
    "usecols = [0, 1, 4, 10]\n",
    "column_names = [\"chrom\", \"start_position\", \"score\", \"fraction_modified\"]\n",
    "\n",
    "# Read the input data, skipping the first row if it's a header or irrelevant\n",
    "df = pd.read_csv(input_path, sep='\\s+', skiprows=1, names=column_names, usecols=usecols)\n",
    "\n",
    "# Create 'coordinate' column for merging\n",
    "df['coordinate'] = df['chrom'].astype(str) + ':' + df['start_position'].astype(str)\n",
    "\n",
    "df_filtered = df.set_index('coordinate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# # read df_discovery and df_validation\n",
    "# df_discovery = pd.read_pickle(mount+'MethylScore/Intermediate_Files/'+'3308samples_333059cpgs_withbatchcorrection_bvalues.pkl').sort_index().iloc[:,1:]\n",
    "# array_reference = pd.read_csv(reference_path, sep='\\t', usecols=['chrm','start','end','probeID']).set_index('probeID')\n",
    "# array_reference['coordinate'] = array_reference['chrm'].astype(str) + ':' + array_reference['start'].astype(str)\n",
    "# pacmap_reference = array_reference.loc[df_discovery.columns]\n",
    "# pacmap_reference.to_pickle(mount+'MethylScore/Intermediate_Files/'+'pacmap_probe_reference.pkl')\n",
    "\n",
    "# # load the reference data\n",
    "# pacmap_reference = pd.read_pickle(mount+'MethylScore_v2/Intermediate_Files/'+'pacmap_probe_reference.pkl').reset_index().set_index('coordinate')\n",
    "\n",
    "# # Save `pacmap_reference` as a .bed file\n",
    "# pacmap_reference[['chrm', 'start', 'end',]].to_csv('~/projects/modkit_runs/pacmap_reference.bed', sep='\\t', header=False, index=False)\n",
    "\n",
    "# # Define columns to be used for the input data\n",
    "# usecols = [0, 1, 4, 10]\n",
    "# column_names = [\"chrom\", \"start_position\", \"score\", \"fraction_modified\"]\n",
    "\n",
    "# # Read the input data, skipping the first row if it's a header or irrelevant\n",
    "# df = pd.read_csv(input_path, sep='\\s+', skiprows=1, names=column_names, usecols=usecols)\n",
    "\n",
    "# # Create 'coordinate' column for merging\n",
    "# df['coordinate'] = df['chrom'].astype(str) + ':' + df['start_position'].astype(str)\n",
    "\n",
    "# df_filtered = df.set_index('coordinate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join with reference data on 'coordinate'\n",
    "df_merged = df_filtered.join(pacmap_reference, how='outer')\n",
    "\n",
    "# Interpolate missing values in 'fraction_modified' column linearly\n",
    "df_merged['fraction_modified'] = df_merged['fraction_modified'].astype(float).interpolate(method='linear')\n",
    "\n",
    "\n",
    "df_merged = df_merged[['fraction_modified']].join(pacmap_reference, how='inner')\n",
    "\n",
    "df_merged = df_merged.drop_duplicates(subset='IlmnID')\n",
    "\n",
    "# Calculate the fraction_modified and prepare the final DataFrame\n",
    "df_merged.loc[:, sample_name] = (df_merged['fraction_modified'] / 100).round(3)\n",
    "\n",
    "df_processed = df_merged[['IlmnID', sample_name]].set_index('IlmnID').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PaCMAP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pacmap\n",
    "\n",
    "def apply_pacmap_model_to_new_data(df, components):\n",
    "\n",
    "    # Load reducer\n",
    "    reducer = pacmap.load(f'../models/pacmap_{components}d_model_al_atlas')\n",
    "\n",
    "    # Project the high dimensional dataset into existing embedding space and return the embedding.\n",
    "    embedding = reducer.transform(df.to_numpy(dtype='float16'))\n",
    "\n",
    "    # Create column names\n",
    "    cols = ['PaCMAP '+ str(i+1) + f' of {components}' for i in range(components)]\n",
    "\n",
    "    # Turn embedding into dataframe\n",
    "    df_embedding = pd.DataFrame(embedding, columns=cols, index=df.index)\n",
    "\n",
    "    return df_embedding\n",
    "\n",
    "df_embedding_2d = apply_pacmap_model_to_new_data(df_processed, 2)\n",
    "df_embedding_5d = apply_pacmap_model_to_new_data(df_processed, 5)\n",
    "\n",
    "df_embedding_2d.to_pickle(output_path + sample_name + '_pacmap_2d.pkl')\n",
    "df_embedding_5d.to_pickle(output_path + sample_name + '_pacmap_5d.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AML Epigenomic Risk</th>\n",
       "      <th>AML Epigenomic Risk P(High Risk)</th>\n",
       "      <th>AL Epigenomic Phenotype</th>\n",
       "      <th>P(AML with inv(16); t(16;16); CBFB::MYH11)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kasumi1_naive</th>\n",
       "      <td>High</td>\n",
       "      <td>0.51</td>\n",
       "      <td>AML with inv(16); t(16;16); CBFB::MYH11</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              AML Epigenomic Risk  AML Epigenomic Risk P(High Risk)  \\\n",
       "kasumi1_naive                High                              0.51   \n",
       "\n",
       "                               AL Epigenomic Phenotype  \\\n",
       "kasumi1_naive  AML with inv(16); t(16;16); CBFB::MYH11   \n",
       "\n",
       "               P(AML with inv(16); t(16;16); CBFB::MYH11)  \n",
       "kasumi1_naive                                       0.636  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load models\n",
    "lgbm_px_model = joblib.load('../models/lgbm_px_model.pkl')\n",
    "lgbm_dx_model = joblib.load('../models/lgbm_dx_model.pkl')\n",
    "\n",
    "# load `df_embedding_5d` from the previous step\n",
    "df_embedding_5d = pd.read_pickle(output_path + sample_name + '_pacmap_5d.pkl')\n",
    "\n",
    "def save_predictions(df, classifier, model_name):\n",
    "\n",
    "    # ignore sklearn warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Select necessary columns\n",
    "    df_features = df.copy()\n",
    "\n",
    "    # Predict using the selected columns\n",
    "    predictions = classifier.predict(df_features)\n",
    "\n",
    "    # Predict probabilities using the selected columns\n",
    "    probabilities = classifier.predict_proba(df_features)\n",
    "\n",
    "    # Convert predictions to a Series with the same index as df_features\n",
    "    predictions_series = pd.Series(predictions, index=df_features.index, name=model_name)\n",
    "\n",
    "    # Convert probabilities to a DataFrame with the same index as df_features and the same columns as the classes\n",
    "    probabilities_df = pd.DataFrame(probabilities, index=df_features.index, columns=classifier.classes_).round(3)\n",
    "\n",
    "    # Add \" - predict_proba\" to the column names\n",
    "    probabilities_df.columns ='P(' + probabilities_df.columns + ')'\n",
    "\n",
    "    # Transform classes of the predictions into integers based on unique values in the classes\n",
    "    probabilities_df[model_name + '_int'] = predictions_series.map({c: i for i, c in enumerate(classifier.classes_)})\n",
    "\n",
    "    # Join predictions with the original DataFrame (already indexed)\n",
    "    df_joined = predictions_series.to_frame().join(probabilities_df)\n",
    "\n",
    "    return df_joined\n",
    "\n",
    "# Execution\n",
    "df_pred_px = save_predictions(df=df_embedding_5d, classifier=lgbm_px_model, model_name='AML Epigenomic Risk')\n",
    "df_pred_dx = save_predictions(df=df_embedding_5d, classifier=lgbm_dx_model, model_name='AL Epigenomic Phenotype')\n",
    "\n",
    "# Map the classes to more desirable labels (low and high risk)\n",
    "df_pred_px['AML Epigenomic Risk'] = df_pred_px['AML Epigenomic Risk'].map({'Alive': 'Low', 'Dead': 'High'})\n",
    "df_pred_px = df_pred_px.rename(columns={'P(Alive)': 'AML Epigenomic Risk P(Low Risk)', 'P(Dead)': 'AML Epigenomic Risk P(High Risk)'})\n",
    "\n",
    "# Join predictions with clinical data\n",
    "df_combined = df_embedding_2d.join(df_embedding_5d).join(df_pred_px).join(df_pred_dx)\n",
    "\n",
    "df_combined[['AML Epigenomic Risk', 'AML Epigenomic Risk P(High Risk)', 'AL Epigenomic Phenotype', f'P({df_combined[\"AL Epigenomic Phenotype\"].item()})']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EWASCox-Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous score cut at the value of 0.4934\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>IlmnID</th>\n",
       "      <th>EWASCox_OS_48CpGs</th>\n",
       "      <th>EWASCox_OS_48CpGs Categorical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kasumi1_naive</th>\n",
       "      <td>2.237542</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "IlmnID         EWASCox_OS_48CpGs EWASCox_OS_48CpGs Categorical\n",
       "kasumi1_naive           2.237542                          High"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from source.cox_lasso import *\n",
    "\n",
    "raw_coefs = pd.read_csv(output_path + 'multivariate_cox_lasso/ewas_cog_os_raw_coefs_newrisk.csv', index_col=0)\n",
    "\n",
    "mean_coefs = set_cutoff(coefs=raw_coefs,threshold=0.99)\n",
    "\n",
    "df_validation = df_processed[mean_coefs.index]\n",
    "\n",
    "df_validation_transformed = df_validation.replace(1, 0.999).replace(0, 0.001)\n",
    "\n",
    "def beta2m(val):\n",
    "    '''Transfrom beta-values into m-values'''\n",
    "    return math.log2(val/(1-val))\n",
    "\n",
    "x_test_m = df_validation_transformed.apply(np.vectorize(beta2m))\n",
    "\n",
    "def standardize_data(df, reference_df):\n",
    "    \"\"\"Standardize data using mean and standard deviation of reference dataset\"\"\"\n",
    "\n",
    "    # Keep only columns that are in both datasets\n",
    "    reference_df = reference_df.loc[:, df.columns]\n",
    "\n",
    "    # Standardize data\n",
    "    df_z = (df - reference_df.mean()) / reference_df.std()\n",
    "\n",
    "    return df_z\n",
    "\n",
    "# Read top CpGs selected from previous code file (univariate cox-ph EWAS)\n",
    "ewas_top_cpgs = pd.read_csv(output_path+'ewas_dmr/ewas_top_cpgs_os.csv', index_col=0)\n",
    "\n",
    "# Standardize data\n",
    "x_test_m_z = standardize_data(df= x_test_m, reference_df= ewas_top_cpgs)\n",
    "\n",
    "score_name = 'EWASCox_OS_48CpGs'\n",
    "\n",
    "df_test, threshold = generate_coxph_score(coef_mean=mean_coefs,\n",
    "                                        x=x_test_m_z,\n",
    "                                        df=df_validation_transformed,\n",
    "                                        score_name=score_name,\n",
    "                                        train_test=0.4934,\n",
    "                                        rpart_outcome='os.time')\n",
    "\n",
    "df_validation_transformed[['EWASCox_OS_48CpGs','EWASCox_OS_48CpGs Categorical']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "df_nanopore = df_combined.join(df_validation_transformed[['EWASCox_OS_48CpGs','EWASCox_OS_48CpGs Categorical']])\n",
    "\n",
    "df_nanopore['Train-Test'] = 'Long-read Nanopore sequencing'\n",
    "df_nanopore['Clinical Trial'] = 'UF Hem Bank'\n",
    "df_nanopore['Patient_ID'] = sample_name\n",
    "df_nanopore['Hematopoietic Entity'] = np.nan\n",
    "df_nanopore['WHO 2022 Diagnosis'] =  np.nan\n",
    "df_nanopore['Vital Status'] = np.nan\n",
    "df_nanopore['Risk Group AAML1831'] = np.nan\n",
    "\n",
    "df_nanopore.to_excel(output_path + sample_name + '_processed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Francisco_Marchi@Lamba_Lab_UF\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.16\n",
      "IPython version      : 8.12.3\n",
      "\n",
      "numpy   : 1.24.4\n",
      "pandas  : 2.0.3\n",
      "pacmap  : 0.7.0\n",
      "sklearn : 1.2.2\n",
      "lightgbm: 3.3.5\n",
      "\n",
      "Compiler    : GCC 11.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.133.1-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 6\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# watermark with all libraries used in this notebook\n",
    "%watermark -v -p numpy,pandas,pacmap,sklearn,lightgbm -a Francisco_Marchi@Lamba_Lab_UF -d -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Please only use the following versions:\n",
    "`python`: 3.8.16\n",
    "`pacmap`: 0.7.0\n",
    "`lightgbm`: 3.3.5\n",
    "`scikit-learn`: 1.2.2\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
