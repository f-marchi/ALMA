{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Please only use the following versions:\n",
    "`python`: 3.8\n",
    "`pacmap`: 0.7.0\n",
    "`lightgbm`: 3.3.5\n",
    "`scikit-learn`: 1.2.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where the data at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mount = '/mnt/e/ALMA/'\n",
    "input_path = mount + 'Intermediate_Files/'\n",
    "output_path = mount + 'Processed_Files/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovery:  3314 samples.\n",
      "Test:        201 samples.\n",
      "Nanopore:     12 samples.\n"
     ]
    }
   ],
   "source": [
    "df_discovery = pd.read_pickle(\n",
    "    input_path+'3314samples_331556cpgs_withbatchcorrection_bvalues.pkl').sort_index().iloc[:,1:]\n",
    "\n",
    "df_test = pd.read_pickle(\n",
    "    input_path+'201samples_331556cpgs_nobatchcorrection_bvalues.pkl').sort_index()[df_discovery.columns]\n",
    "\n",
    "df_nanopore_test = pd.read_pickle(\n",
    "    input_path+'12samples_331556cpgs_nanoporeseq_bvalues.pkl').sort_index()[df_discovery.columns]\n",
    "\n",
    "print(\n",
    "f'Discovery:  {df_discovery.shape[0]} samples.\\nTest:        {df_test.shape[0]} samples.\\nNanopore:     {df_nanopore_test.shape[0]} samples.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality with PaCMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __PaCMAP__: Large-scale Dimension Reduction Technique Preserving Both Global and Local Structure\n",
    "\n",
    "- __Github__: [https://github.com/YingfanWang/PaCMAP](https://github.com/YingfanWang/PaCMAP)\n",
    "\n",
    "- __Benchmarking__: [Towards a comprehensive evaluation of dimension reduction methods for transcriptomic data visualization](https://pubmed.ncbi.nlm.nih.gov/35853932/)\n",
    "\n",
    "- __Original Paper__: [Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization](https://jmlr.org/papers/v22/20-1061.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters, fit, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaCMAP version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "import pacmap\n",
    "print('PaCMAP version:', pacmap.__version__)\n",
    "\n",
    "# components_list = [2,5]\n",
    "# for components in components_list:\n",
    "    \n",
    "#     reducer = pacmap.PaCMAP(n_components=components, \n",
    "#                             n_neighbors=10, \n",
    "#                             MN_ratio=0.4, \n",
    "#                             FP_ratio=20.0, \n",
    "#                             lr=0.1, \n",
    "#                             num_iters=4500,\n",
    "#                             random_state=42,\n",
    "#                             save_tree=True)\n",
    "\n",
    "#     # Project the high dimensional dataset into a low-dimensional embedding\n",
    "#     embedding_training = reducer.fit_transform(df_discovery.to_numpy(dtype='float16'))\n",
    "\n",
    "#     # Save reducer\n",
    "#     pacmap.save(reducer, output_path + f'pacmap/pacmap_{components}d_model_alma')\n",
    "\n",
    "#     # Create column names\n",
    "#     cols = ['PaCMAP '+ str(i+1) + f' of {components}' for i in range(components)]\n",
    "\n",
    "#     # Turn embedding into dataframe\n",
    "#     df_embedding = pd.DataFrame(embedding_training, columns=cols, index=df_discovery.index).to_pickle(\n",
    "#         output_path+f'pacmap/pacmap_{components}d_embedding.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply models to new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply_pacmap_model_to_new_data(df, components):\n",
    "\n",
    "    # Load reducer\n",
    "    reducer = pacmap.load(output_path + f'pacmap/pacmap_{components}d_model_alma')\n",
    "\n",
    "    # Project the high dimensional dataset into existing embedding space and return the embedding.\n",
    "    embedding = reducer.transform(df.to_numpy(dtype='float16'))\n",
    "\n",
    "    # Create column names\n",
    "    cols = ['PaCMAP '+ str(i+1) + f' of {components}' for i in range(components)]\n",
    "\n",
    "    # Turn embedding into dataframe\n",
    "    df_embedding = pd.DataFrame(embedding, columns=cols, index=df.index)\n",
    "\n",
    "    return df_embedding\n",
    "\n",
    "\n",
    "# Load pacmap results applied to discovery dataset\n",
    "train_2d = pd.read_pickle(output_path+'pacmap/pacmap_2d_embedding.pkl')\n",
    "train_5d = pd.read_pickle(output_path+'pacmap/pacmap_5d_embedding.pkl')\n",
    "\n",
    "# Apply pacmap model to validation dataset\n",
    "test_2d = apply_pacmap_model_to_new_data(df_test, components=2)\n",
    "test_5d = apply_pacmap_model_to_new_data(df_test, components=5)\n",
    "\n",
    "# Apply pacmap model to test dataset\n",
    "test_nano_2d = apply_pacmap_model_to_new_data(df_nanopore_test, components=2)\n",
    "test_nano_5d = apply_pacmap_model_to_new_data(df_nanopore_test, components=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge results with clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Join 2d and 5d\n",
    "train = train_2d.join(train_5d)\n",
    "test = test_2d.join(test_5d)\n",
    "test_nano = test_nano_2d.join(test_nano_5d)\n",
    "\n",
    "# Concatenate train and test\n",
    "train_test = pd.concat([train, test, test_nano])\n",
    "\n",
    "# Read clinical data\n",
    "clinical_data = pd.read_excel(input_path+'clinical_data.xlsx', index_col=0)\n",
    "\n",
    "# Join train_test with clinical data\n",
    "df = train_test.join(clinical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select samples for prognostic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select samples that:\n",
    "- Have `vital status` and `time to death` information available publicly.\n",
    "- Are from `AAML1031` and `AAML0531` pediatric AML trials.\n",
    "- Are diagnostic from BM or PB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381 samples removed. 1146 samples remaining.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train-Test\n",
       "Train Sample    946\n",
       "Test Sample     200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the samples with missing labels for the selected column\n",
    "df_px = df[~df['Vital Status at 5y'].isna()]\n",
    "\n",
    "# Select only the samples from the following clinical trials\n",
    "df_px2 = df_px[df_px['Clinical Trial'].isin(['AAML0531', 'AAML1031', 'AAML03P1', 'AML02', 'AML08'])]\n",
    "\n",
    "# Select only diagnostic samples\n",
    "df_px2 = df_px2[df_px2['Sample Type'].isin(\n",
    "    ['Diagnosis', 'Primary Blood Derived Cancer - Bone Marrow', 'Primary Blood Derived Cancer - Peripheral Blood'])]\n",
    "\n",
    "# Remove duplicates\n",
    "df_px2 = df_px2[~df_px2['Patient_ID'].duplicated(keep='last')]\n",
    "\n",
    "# print the number of samples dropped and the amount remaining\n",
    "print(df.shape[0]-df_px2.shape[0], 'samples removed.'\\\n",
    ", df_px2.shape[0], 'samples remaining.')\n",
    "\n",
    "df_px2['Train-Test'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select samples Dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select samples that:\n",
    "- Have `WHO 2022 Diagnosis` information available publicly.\n",
    "- Belong to diagnostic subtypes with more than five samples (due to 5-fold CV)\n",
    "    - The subtype `AML with t(9;22); BCR::ABL1` was excluded due to having only three samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "See chapter on clinical data processing for how our rigorous programatic reclassification to WHO 2022 was performed.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 samples removed. 2577 samples remaining.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Train-Test\n",
       "Train Sample    2471\n",
       "Test Sample      106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the samples with missing labels for the ELN AML 2022 Diagnosis\n",
    "df_dx = df[~df['WHO 2022 Diagnosis'].isna()]\n",
    "\n",
    "# exclude the classes with fewer than 10 samples\n",
    "df_dx2 = df_dx[~df_dx['WHO 2022 Diagnosis'].isin(['AML with t(9;22); BCR::ABL1'])]\n",
    "\n",
    "# print the number of samples dropped and the amount remaining\n",
    "print(df.shape[0]-df_dx2.shape[0], 'samples removed.'\\\n",
    ", df_dx2.shape[0], 'samples remaining.')\n",
    "\n",
    "df_dx2['Train-Test'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2471, 5) X_test shape: (106, 5)\n",
      "X_train shape: (946, 5) X_test shape: (200, 5)\n"
     ]
    }
   ],
   "source": [
    "def custom_train_test_split(df, feature_columns, target_column, split_column):\n",
    "\n",
    "    X = df[feature_columns].to_numpy(dtype='float16')\n",
    "    y = df[target_column].to_numpy()\n",
    "\n",
    "    train_mask = df[split_column] == 'Train Sample'\n",
    "    test_mask = df[split_column] == 'Test Sample'\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "    print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Execution\n",
    "X_train_dx, X_test_dx, y_train_dx, y_test_dx = custom_train_test_split(df_dx2, test_5d.columns,'WHO 2022 Diagnosis', 'Train-Test')\n",
    "X_train_px, X_test_px, y_train_px, y_test_px = custom_train_test_split(df_px2, test_5d.columns,'Vital Status at 5y', 'Train-Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __LightGBM__: A fast, distributed, high performance gradient boosting framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n",
    "\n",
    "- __Github__: [https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)\n",
    "\n",
    "- __Original Paper__: [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'class_weight': 'balanced', 'reg_alpha': 0.1, 'reg_lambda': 4}\n",
      "Best parameters: {'class_weight': 'balanced', 'reg_alpha': 8, 'reg_lambda': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/lgbm_dx_model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "def benchmark_classifier(X_train, y_train):\n",
    "\n",
    "    param_grid = {\n",
    "    # 'num_leaves': [ 5, 6, 7, 8, 9, 10], # number of leaves in full tree\n",
    "    # 'max_depth': [3,4,5,6,7,8,9,10],  # maximum depth of a tree\n",
    "    # 'learning_rate': [0.01],  # shrinkage rate\n",
    "    # 'n_estimators': [50, 100, 500],  # number of trees (or rounds)\n",
    "    'reg_alpha': [0.1,1,2,4,6,8],  # L1 regularization\n",
    "    'reg_lambda': [0.1,1,2,4],  # L2 regularization\n",
    "    'class_weight': ['balanced'],  # weights associated with classes    \n",
    "    }\n",
    "\n",
    "    # Initialize the LGBM Classifier with regularization\n",
    "    lgbm = LGBMClassifier(random_state=42, n_jobs=-1) \n",
    "                          \n",
    "    # Perform grid search with stratified cross-validation\n",
    "    grid_search = GridSearchCV(lgbm, param_grid, cv=5, n_jobs=-1,\n",
    "                               scoring='roc_auc_ovr_weighted')\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train, \n",
    "                    \n",
    "                    )\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    # Get the best model\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    return clf\n",
    "\n",
    "# Benchmark, train\n",
    "lgbm_dx_model = benchmark_classifier(X_train_dx, y_train_dx)\n",
    "lgbm_px_model = benchmark_classifier(X_train_px, y_train_px)\n",
    "\n",
    "# Save models\n",
    "joblib.dump(lgbm_px_model, '../models/lgbm_px_model.pkl')\n",
    "joblib.dump(lgbm_dx_model, '../models/lgbm_dx_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models created**:\n",
    "- **AL Epigenomic Subtype**: Predicts 27 WHO 2022 subtypes of acute leukemia + otherwise-normal control.\n",
    "- **AML Epigenomic Risk**: Predicts the probability of death within 5 years for AML patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ../data/alma_main_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies again in case only this cell is run\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load models\n",
    "lgbm_px_model = joblib.load('../models/lgbm_px_model.pkl')\n",
    "lgbm_dx_model = joblib.load('../models/lgbm_dx_model.pkl')\n",
    "\n",
    "def save_predictions(df, classifier, model_name):\n",
    "    # ignore sklearn warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # Select necessary columns\n",
    "    df_features = df[test_5d.columns]\n",
    "\n",
    "    # Predict using the selected columns\n",
    "    predictions = classifier.predict(df_features)\n",
    "\n",
    "    # Predict probabilities using the selected columns\n",
    "    probabilities = classifier.predict_proba(df_features)\n",
    "\n",
    "    # Convert predictions to a Series with the same index as df_features\n",
    "    predictions_series = pd.Series(predictions, index=df_features.index, name=model_name)\n",
    "\n",
    "    # Convert probabilities to a DataFrame with the same index as df_features and the same columns as the classes\n",
    "    probabilities_df = pd.DataFrame(probabilities, index=df_features.index, columns=classifier.classes_).round(3)\n",
    "\n",
    "    # Add \" - predict_proba\" to the column names\n",
    "    probabilities_df.columns = 'P(' + probabilities_df.columns + ')'\n",
    "\n",
    "    # Transform classes of the predictions into integers based on unique values in the classes\n",
    "    probabilities_df[model_name + '_int'] = predictions_series.map({c: i for i, c in enumerate(classifier.classes_)})\n",
    "\n",
    "    # Replace predicted label with nan when probability is <PROB \n",
    "    # (if the model is not confident, then the prediction is not reliable and will not be used)\n",
    "    PROB = 0.5\n",
    "    max_prob = probabilities_df.iloc[:, :len(classifier.classes_)].max(axis=1)\n",
    "    predictions_series[max_prob < PROB] = np.nan\n",
    "\n",
    "    # Add a new column for confidence of predictions\n",
    "    probabilities_df[model_name + ' >' + str(PROB*100) + '% Confidence'] = max_prob >= PROB\n",
    "\n",
    "    # Join predictions with the original DataFrame (already indexed)\n",
    "    df_joined = predictions_series.to_frame().join(probabilities_df)\n",
    "\n",
    "    return df_joined\n",
    "\n",
    "# Execution\n",
    "df_pred_px = save_predictions(df=df, classifier=lgbm_px_model, model_name='AML Epigenomic Risk')\n",
    "df_pred_dx = save_predictions(df=df, classifier=lgbm_dx_model, model_name='AL Epigenomic Subtype')\n",
    "\n",
    "# Map the classes to more desirable labels (low and high risk)\n",
    "df_pred_px['AML Epigenomic Risk'] = df_pred_px['AML Epigenomic Risk'].map({'Alive': 'Low', 'Dead': 'High'})\n",
    "df_pred_px = df_pred_px.rename(columns={'P(Alive)': 'P(Remission) at 5y', 'P(Dead)': 'P(Death) at 5y'})\n",
    "\n",
    "# Remove ALL samples from df_pred_px since they should not be part of the AML Epigenomic Risk\n",
    "lymphoblastic_leukemia_samples = df[df['Clinical Trial'].isin(['NOPHO ALL92-2000', 'French GRAALL 2003–2005', 'TARGET ALL'])].index\n",
    "df_pred_px = df_pred_px.drop(lymphoblastic_leukemia_samples)\n",
    "\n",
    "# Join predictions with clinical data\n",
    "df_combined = df.join(df_pred_px).join(df_pred_dx)\n",
    "\n",
    "# Add \"Dx Prediction Concordance\" column\n",
    "def check_concordance(row):\n",
    "    who_dx = row['WHO 2022 Diagnosis']\n",
    "    al_risk = row['AL Epigenomic Subtype']\n",
    "    if pd.notnull(who_dx) and pd.notnull(al_risk):\n",
    "        return who_dx == al_risk\n",
    "    return np.nan\n",
    "\n",
    "df_combined['Dx Prediction Concordance'] = df_combined.apply(check_concordance, axis=1)\n",
    "\n",
    "# Save df_combined\n",
    "df_combined.to_excel('../data/alma_main_results.xlsx')\n",
    "print('Dataset saved to ../data/alma_main_results.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Francisco_Marchi@Lamba_Lab_UF\n",
      "\n",
      "Last updated: 2024-09-20\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.18\n",
      "IPython version      : 8.12.3\n",
      "\n",
      "numpy   : 1.24.4\n",
      "pandas  : 2.0.3\n",
      "pacmap  : 0.7.0\n",
      "sklearn : 1.2.2\n",
      "lightgbm: 3.3.5\n",
      "\n",
      "Compiler    : GCC 11.4.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.133.1-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 32\n",
      "Architecture: 64bit\n",
      "\n",
      "Git repo: git@github.com:f-marchi/ALMA.git\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# watermark with all libraries used in this notebook and add date\n",
    "%watermark -v -p numpy,pandas,pacmap,sklearn,lightgbm -a Francisco_Marchi@Lamba_Lab_UF -d -m -r -u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
